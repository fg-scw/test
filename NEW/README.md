# Talos Kubernetes sur Scaleway

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Talos](https://img.shields.io/badge/Talos-v1.11.5-blue.svg)](https://www.talos.dev/)
[![Kubernetes](https://img.shields.io/badge/Kubernetes-v1.31-blue.svg)](https://kubernetes.io/)

DÃ©ploiement automatisÃ© d'un cluster Kubernetes hautement disponible avec **Talos Linux** sur le cloud **Scaleway**.

Ce projet construit une infrastructure complÃ¨te pour exÃ©cuter Kubernetes sur Scaleway avec :

- **Talos Linux** : OS immuable et API-driven pour Kubernetes
- **Terraform** : Infrastructure as Code pour le provisioning
- **Packer** : Automatisation de la crÃ©ation d'images personnalisÃ©es
- **Cilium** : CNI moderne basÃ© sur eBPF avec kube-proxy replacement
- **VPC & Private Networks** : RÃ©seau isolÃ© avec IPAM automatique

## ğŸ“‹ Table des matiÃ¨res

* [ğŸ— Architecture](#-architecture)
* [ğŸ”§ PrÃ©requis](#-prÃ©requis)
* [ğŸš€ DÃ©marrage rapide](#-dÃ©marrage-rapide)
* [ğŸ“ Structure du projet](#-structure-du-projet)
* [ğŸ“š Documentation](#-documentation)
* [ğŸ¤ Contribution](#-contribution)
* [ğŸ“ Licence](#-licence)

## ğŸ— Architecture

### Topologie rÃ©seau

L'infrastructure repose sur une architecture multi-AZ hautement disponible avec VPC Scaleway :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Scaleway Cloud (fr-par)                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ VPC Kubernetes (Regional)                                      â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚ Private Network (10.0.0.0/22) - IPAM automatique         â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ PAR-1        â”‚  â”‚ PAR-2        â”‚  â”‚ PAR-3        â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ CP-1         â”‚  â”‚ CP-2         â”‚  â”‚ CP-3         â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â”‚ Worker-1     â”‚  â”‚ Worker-2     â”‚  â”‚ Worker-3     â”‚  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â”‚
â”‚  â”‚  â”‚                                                          â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  Load Balancer (Internal) - API Kubernetes                     â”‚  â”‚
â”‚  â”‚  â”œâ”€> Control-Plane-1:6443 (PAR-1)                              â”‚  â”‚
â”‚  â”‚  â”œâ”€> Control-Plane-2:6443 (PAR-2)                              â”‚  â”‚
â”‚  â”‚  â””â”€> Control-Plane-3:6443 (PAR-3)                              â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  Public Gateway - NAT pour accÃ¨s internet                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                      â”‚
â”‚  Bastion (VPC sÃ©parÃ©) - Point d'entrÃ©e administration              â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Composants

**RÃ©seau Scaleway**

- VPC rÃ©gional (fr-par)
- Private Network avec IPAM automatique (/22 = 1020 IPs)
- Public Gateway pour NAT (accÃ¨s internet sortant)
- Load Balancer interne pour API Kubernetes
- DNS interne automatique (resources.pn.internal)

**CNI & RÃ©seau Kubernetes**

- CNI : Cilium (eBPF, native routing mode)
- Pod CIDR : `10.244.0.0/16`
- Service CIDR : `10.96.0.0/12`
- kube-proxy : DÃ©sactivÃ© (remplacÃ© par Cilium)
- MTU : 1500 (default Scaleway Private Network)

**Control Plane (3 nÅ“uds)**

- Distribution : 3 zones de disponibilitÃ© (PAR-1, PAR-2, PAR-3)
- Type : `PRO2-S` (2 vCPU, 8 GB RAM) ou `DEV1-M` (test)
- RÃ´les : etcd, kube-apiserver, kube-controller-manager, kube-scheduler
- HA : Quorum etcd 3 nÅ“uds (tolÃ¨re 1 panne)
- Stockage : SBS 5K, 50 GB minimum

**Workers (3+ nÅ“uds)**

- Distribution : 3 zones de disponibilitÃ© minimum
- Type : Configurable selon les charges (PRO2-M, PRO2-L recommandÃ©s)
- RÃ´les : HÃ©bergement des workloads Kubernetes
- Stockage : SBS 5K, 100 GB recommandÃ©

**Stockage**

- Type : Block Storage SBS (NVMe)
- SBS 5K : 5000 IOPS garantis (production standard)
- SBS 15K : 15000 IOPS garantis (haute performance)
- Persistence : Volumes indÃ©pendants des instances

## ğŸ”§ PrÃ©requis

### Outils requis

Sur votre poste de travail :

```bash
# Terraform (>= 1.9)
terraform version

# Packer (>= 1.11)
packer version

# talosctl (version correspondant Ã  Talos)
talosctl version

# kubectl
kubectl version --client

# helm (optionnel, pour Cilium)
helm version

# scw CLI (Scaleway CLI)
scw version
```

### âš ï¸ Important : Types de volumes Scaleway

Ce projet utilise les types de volumes appropriÃ©s pour chaque cas :
- **Packer** : `l_ssd` (Local SSD pour instances temporaires)
- **Production** : `sbs_5k` (Block Storage NVMe 5K IOPS)
- **Bastion** : `l_ssd` (Local SSD suffit)

Voir [VOLUME_TYPES.md](VOLUME_TYPES.md) pour plus de dÃ©tails.

### Credentials Scaleway

Vous aurez besoin de :

- Un projet Scaleway
- AccÃ¨s API (Secret Key et Access Key)
- Permissions pour :
  - CrÃ©er des VPC et Private Networks
  - CrÃ©er des instances et volumes
  - CrÃ©er des images et snapshots
  - GÃ©rer les Load Balancers et Public Gateways
  - AccÃ¨s Object Storage (pour import d'images)

### Versions testÃ©es

| Composant | Version |
|-----------|---------|
| Talos Linux | v1.11.5 |
| Kubernetes | v1.31.1 |
| Cilium | v1.16+ |
| Terraform | v1.9+ |
| Packer | v1.11+ |
| Provider Scaleway | v2.62+ |

## ğŸš€ DÃ©marrage rapide

### 1. Configuration des credentials

Clonez le repository et configurez vos credentials :

```bash
git clone https://github.com/votrecompte/talos-scaleway.git
cd talos-scaleway

# Copier le fichier d'exemple
cp .envrc.sample .envrc

# Ã‰diter avec vos credentials
vim .envrc
```

Contenu de `.envrc` :

```bash
export SCW_ACCESS_KEY="VOTRE_ACCESS_KEY"
export SCW_SECRET_KEY="VOTRE_SECRET_KEY"
export SCW_DEFAULT_PROJECT_ID="VOTRE_PROJECT_ID"
export SCW_DEFAULT_REGION="fr-par"
export SCW_DEFAULT_ZONE="fr-par-1"

export TF_VAR_scw_access_key="$SCW_ACCESS_KEY"
export TF_VAR_scw_secret_key="$SCW_SECRET_KEY"
export TF_VAR_scw_project_id="$SCW_DEFAULT_PROJECT_ID"

export PACKER_LOG=1
export PACKER_LOG_PATH="./packer.log"
```

Chargez les variables :

```bash
source .envrc
# Ou avec direnv
direnv allow
```

### 2. CrÃ©ation de l'image Talos

L'image Talos est crÃ©Ã©e via Packer en utilisant une instance temporaire :

```bash
cd packer

# Initialiser Packer
packer init .

# Valider la configuration
packer validate -var="talos_version=v1.11.5" .

# Build de l'image
packer build -var="talos_version=v1.11.5" .
```

L'image crÃ©Ã©e aura un nom comme : `talos-scaleway-v1.11.5-20251114-081824`

**Note** : Packer crÃ©era automatiquement :
1. Une instance temporaire
2. TÃ©lÃ©chargera l'image Talos depuis Image Factory
3. CrÃ©era un snapshot
4. GÃ©nÃ©rera une image rÃ©utilisable
5. Nettoiera les ressources temporaires

### 3. DÃ©ploiement de l'infrastructure

```bash
cd terraform

# Copier le fichier d'exemple de variables
cp terraform.tfvars.example terraform.tfvars

# Ã‰diter avec vos paramÃ¨tres (notamment l'image ID crÃ©Ã©e prÃ©cÃ©demment)
vim terraform.tfvars

# Initialiser Terraform
terraform init

# Planifier les changements
terraform plan

# Appliquer
terraform apply
```

### 4. Bootstrap du cluster Kubernetes

Une fois l'infrastructure dÃ©ployÃ©e :

```bash
# RÃ©cupÃ©rer les outputs Terraform
export CONTROL_PLANE_IP=$(terraform output -raw control_plane_lb_ip)

# GÃ©nÃ©rer les configurations Talos
talosctl gen config talos-cluster https://${CONTROL_PLANE_IP}:6443 \
  --output-dir _out \
  --with-docs=false \
  --with-examples=false

# Appliquer patch pour dÃ©sactiver kube-proxy (Cilium le remplace)
talosctl --talosconfig _out/talosconfig machineconfig patch \
  _out/controlplane.yaml \
  --patch @cilium-patch.yaml \
  -o _out/controlplane-patched.yaml

# Appliquer les configurations aux control planes
for ip in $(terraform output -json control_plane_ips | jq -r '.[]'); do
  talosctl apply-config --insecure \
    --nodes $ip \
    --file _out/controlplane-patched.yaml
done

# Appliquer les configurations aux workers
for ip in $(terraform output -json worker_ips | jq -r '.[]'); do
  talosctl apply-config --insecure \
    --nodes $ip \
    --file _out/worker.yaml
done

# Bootstrap etcd sur le premier control plane
FIRST_CP=$(terraform output -json control_plane_ips | jq -r '.[0]')
talosctl --talosconfig _out/talosconfig \
  bootstrap --nodes $FIRST_CP

# RÃ©cupÃ©rer le kubeconfig
talosctl --talosconfig _out/talosconfig \
  kubeconfig _out/kubeconfig --nodes $CONTROL_PLANE_IP

# VÃ©rifier le cluster
export KUBECONFIG=_out/kubeconfig
kubectl get nodes
```

### 5. Installation de Cilium

```bash
# Ajouter le repo Helm de Cilium
helm repo add cilium https://helm.cilium.io/
helm repo update

# Installer Cilium
helm install cilium cilium/cilium \
  --namespace kube-system \
  --set ipam.mode=kubernetes \
  --set kubeProxyReplacement=true \
  --set k8sServiceHost=$CONTROL_PLANE_IP \
  --set k8sServicePort=6443

# VÃ©rifier l'installation
kubectl -n kube-system get pods -l k8s-app=cilium
cilium status --wait
```

ğŸ‰ **Votre cluster Kubernetes Talos sur Scaleway est opÃ©rationnel !**

## ğŸ“ Structure du projet

```text
.
â”œâ”€â”€ README.md                      # Ce fichier
â”œâ”€â”€ .envrc.sample                  # Template de credentials
â”œâ”€â”€ cilium-patch.yaml              # Patch Talos pour dÃ©sactiver kube-proxy
â”œâ”€â”€ packer/
â”‚   â”œâ”€â”€ talos-scaleway.pkr.hcl    # Configuration Packer
â”‚   â”œâ”€â”€ provision/
â”‚   â”‚   â”œâ”€â”€ build-image.sh         # Script de build de l'image
â”‚   â”‚   â””â”€â”€ schematic.yaml         # Schematic Talos personnalisÃ©
â”‚   â””â”€â”€ variables.pkr.hcl          # Variables Packer
â”œâ”€â”€ terraform/
â”‚   â”œâ”€â”€ main.tf                    # Configuration Terraform principale
â”‚   â”œâ”€â”€ variables.tf               # Variables d'entrÃ©e
â”‚   â”œâ”€â”€ outputs.tf                 # Outputs exposÃ©s
â”‚   â”œâ”€â”€ versions.tf                # Provider versions
â”‚   â”œâ”€â”€ vpc.tf                     # VPC et Private Networks
â”‚   â”œâ”€â”€ compute.tf                 # Instances Talos
â”‚   â”œâ”€â”€ security_groups.tf         # Security Groups
â”‚   â”œâ”€â”€ load_balancer.tf           # Load Balancer API Kubernetes
â”‚   â”œâ”€â”€ public_gateway.tf          # Public Gateway pour NAT
â”‚   â””â”€â”€ terraform.tfvars.example   # Exemple de variables
â””â”€â”€ _out/                          # Outputs gÃ©nÃ©rÃ©s
    â”œâ”€â”€ talosconfig
    â”œâ”€â”€ kubeconfig
    â”œâ”€â”€ controlplane.yaml
    â””â”€â”€ worker.yaml
```

## ğŸ“š Documentation

### Documentation Scaleway

- **VPC & Private Networks** : https://www.scaleway.com/en/docs/vpc/
- **IPAM** : https://www.scaleway.com/en/docs/vpc/reference-content/understanding-ipam/
- **Instances** : https://www.scaleway.com/en/docs/compute/instances/
- **Load Balancers** : https://www.scaleway.com/en/docs/network/load-balancer/
- **Public Gateway** : https://www.scaleway.com/en/docs/public-gateways/

### Documentation Talos & Kubernetes

- **Talos officiel** : https://www.talos.dev/
- **Talos Image Factory** : https://factory.talos.dev/
- **Cilium** : https://docs.cilium.io/
- **Terraform Scaleway Provider** : https://registry.terraform.io/providers/scaleway/scaleway/

### SpÃ©cificitÃ©s Scaleway

#### Private Networks et IPAM

- Les Private Networks sont **rÃ©gionaux** et couvrent automatiquement toutes les AZ
- IPAM alloue automatiquement les IPs privÃ©es (pas de DHCP Ã  configurer)
- CIDR par dÃ©faut : /22 (1020 IPs utilisables)
- DNS interne : `<resource-name>.<private-network-name>.internal`

#### Multi-AZ

- PAR-1, PAR-2, PAR-3 disponibles dans la rÃ©gion Paris
- Les instances sont zonales mais peuvent communiquer via le Private Network rÃ©gional
- Les volumes Block Storage sont zonaux (non migrables entre AZ)

#### Public Gateway

- Fournit NAT pour accÃ¨s internet sortant
- Mode IPAM obligatoire (legacy deprecated)
- Peut attacher jusqu'Ã  8 Private Networks
- SSH Bastion intÃ©grÃ© disponible

## ğŸ¤ Contribution

Les contributions sont les bienvenues !

1. Forker le projet
2. CrÃ©er une branche feature (`git checkout -b feature/AmazingFeature`)
3. Commiter vos changements (`git commit -m 'Add some AmazingFeature'`)
4. Pousser vers la branche (`git push origin feature/AmazingFeature`)
5. Ouvrir une Pull Request

## ğŸ“ Licence

Ce projet est distribuÃ© sous licence Apache 2.0. Voir le fichier [LICENSE](LICENSE) pour plus de dÃ©tails.

## ğŸ‘¤ Auteur

**Adaptation Scaleway**

BasÃ© sur le travail original pour Outscale par StÃ©phane Robert

---

â­ **Si ce projet vous est utile, n'hÃ©sitez pas Ã  lui mettre une star !**
