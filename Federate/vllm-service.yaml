apiVersion: ray.io/v1
kind: RayService
metadata:
  name: vllm-federated-service
  namespace: default
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300

  # Ray Serve / vLLM configuration
  serveConfigV2: |
    applications:
      - name: vllm
        import_path: vllm.entrypoints.openai.api_server:build_app
        route_prefix: "/"
        runtime_env:
          pip:
            - "vllm==0.6.0"
            - "transformers==4.44.0"
        deployments:
          - name: vllm
            num_replicas: 1
            max_concurrent_queries: 8
            user_config:
              model: "mistralai/Mistral-7B-Instruct-v0.2"
              tensor_parallel_size: 1
              trust_remote_code: true

  rayClusterConfig:
    rayVersion: "2.30.0"

    headGroupSpec:
      serviceType: ClusterIP
      rayStartParams:
        dashboard-host: "0.0.0.0"
      template:
        spec:
          nodeSelector:
            ray-node-type: "head"
          containers:
            - name: ray-head
              image: "rayproject/ray-ml:2.30.0-gpu"
              resources:
                requests:
                  cpu: "2"
                  memory: "4Gi"
                limits:
                  cpu: "4"
                  memory: "8Gi"
              ports:
                - containerPort: 8265
                  name: dashboard

    # One worker group per AZ, each bound to a GPU node via nodeSelector
    workerGroupSpecs:
      - groupName: gpu-fr-par-1
        replicas: 1
        minReplicas: 0
        maxReplicas: 5
        rayStartParams:
          num-gpus: "1"
        template:
          spec:
            nodeSelector:
              topology.kubernetes.io/zone: "fr-par-1"
              nvidia.com/gpu.present: "true"
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "node.kubernetes.io/disk-pressure"
                operator: "Exists"
                effect: "NoSchedule"
            containers:
              - name: ray-worker
                image: "rayproject/ray-ml:2.30.0-gpu"
                resources:
                  requests:
                    cpu: "4"
                    memory: "24Gi"
                    nvidia.com/gpu: 1
                  limits:
                    cpu: "4"
                    memory: "24Gi"
                    nvidia.com/gpu: 1

      - groupName: gpu-fr-par-2
        replicas: 1
        minReplicas: 0
        maxReplicas: 5
        rayStartParams:
          num-gpus: "1"
        template:
          spec:
            nodeSelector:
              topology.kubernetes.io/zone: "fr-par-2"
              nvidia.com/gpu.present: "true"
            tolerations:
              - key: "nvidia.com/gpu"
                operator: "Exists"
                effect: "NoSchedule"
              - key: "node.kubernetes.io/disk-pressure"
                operator: "Exists"
                effect: "NoSchedule"
            containers:
              - name: ray-worker
                image: "rayproject/ray-ml:2.30.0-gpu"
                resources:
                  requests:
                    cpu: "4"
                    memory: "24Gi"
                    nvidia.com/gpu: 1
                  limits:
                    cpu: "4"
                    memory: "24Gi"
                    nvidia.com/gpu: 1

      # - groupName: gpu-pl-waw-2
      #   replicas: 1
      #   minReplicas: 0
      #   maxReplicas: 5
      #   rayStartParams:
      #     num-gpus: "1"
      #   template:
      #     spec:
      #       nodeSelector:
      #         topology.kubernetes.io/zone: "pl-waw-2"
      #         nvidia.com/gpu.present: "true"
      #       tolerations:
      #         - key: "nvidia.com/gpu"
      #           operator: "Exists"
      #           effect: "NoSchedule"
      #       containers:
      #         - name: ray-worker
      #           image: "rayproject/ray-ml:2.30.0-gpu"
      #           resources:
      #             requests:
      #               cpu: "4"
      #               memory: "24Gi"
      #               nvidia.com/gpu: 1
      #             limits:
      #               cpu: "4"
      #               memory: "24Gi"
      #               nvidia.com/gpu: 1
